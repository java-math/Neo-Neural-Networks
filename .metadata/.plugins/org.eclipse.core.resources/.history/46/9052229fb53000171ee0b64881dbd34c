package net.mileselvidge.neo;


import java.lang.reflect.Method;

/*
 * Neo: Java Machine Learning Library
 * A Machine Learning library for Java containing Neural Networks and Matrix Mathematics
 * by Miles Elvidge, 17
 * Contact: miles.elvidge@ntlworld.com
 * Started 01/05/17
 */

public class DeepNeuralNetwork {
	// A not so basic deep neural network structure
	
	// Number of nodes in different layers
	private int[] layers;
	
	// Functions for calculating activation and derivatives
	private Method activation;
	private Method derivative;
	
	// Weight Matrices
	private NMatrix[] weights;
	private NMatrix wIH;
	private NMatrix wHO;
	
	public double learningRate;
	
	// Constructors 

	// Initialise from scratch 
	public DeepNeuralNetwork(int inputSize, int hiddenSizes[], int outputSize, double lr, String activationType) {
		// Initialise layers of neural network
		this.layers = new int[hiddenSizes.length+2];
		
		// Initialise input/output layers of the neural network
		this.layers[0] = inputSize;
		this.layers[this.layers.length-1] = outputSize;
		
		// Initialise hidden layers
		for(int i = 1; i < this.layers.length-1; i++) {
			this.layers[i] = hiddenSizes[i-1];
		}
		
		this.learningRate = lr;
		
		// Initialise weights with random gaussian values
		weights = new NMatrix[this.layers.length-1];
		for(int i = 0; i < weights.length; i++) {
			weights[i] = NMatrix.random(this.layers[i+1], this.layers[i]);
 		}
		
		// Initialise the activation/derivative functions
		NMath math = new NMath();
		try {
			if(activationType.toLowerCase() == "tanh"){
				activation = math.getClass().getMethod("tanh", Double.TYPE);
				derivative = math.getClass().getMethod("dtanh", Double.TYPE);
			} else {
				// Default
				activation = math.getClass().getMethod("Sigmoid", Double.TYPE);
				derivative = math.getClass().getMethod("dSigmoid", Double.TYPE);
			}
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}
	
	// Initialise a neural network from a neural network
	public DeepNeuralNetwork(DeepNeuralNetwork net){
		this.layers = net.layers;
		this.learningRate = net.learningRate;
		this.weights = net.weights;
		this.activation = net.activation;
		this.derivative = net.derivative;
	}
	
	public DeepNeuralNetwork copy() {
		return new DeepNeuralNetwork(this); 
	}
	
	// Query the network
	public double[] query(double[] inputsArr){
		try {
			// Convert inputs array to a matrix
			NMatrix inputs = NMatrix.fromArray(inputsArr);
			NMatrix current = inputs.copy();
			
			for(int i = 0; i < weights.length; i++) {
				NMatrix hiddenLayer = NMatrix.multiply(weights[i], current);
				current = NMatrix.map(hiddenLayer, this, this.activation);
			}
			
			return current.transpose().asArray()[0]; // Outputs
		} catch (Exception e) {
			System.out.println("Fatal Error occoured: contact developer!");
			return new double[] {0};
		}
	}
	
	// Train the neural network
	public void train(double[] inputsArr, double[] targetArr) {
		try {
			// Turn inputs & targets in matrices
			NMatrix inputs = NMatrix.fromArray(inputsArr);
			NMatrix targets = NMatrix.fromArray(targetArr);
			
			NMatrix current = inputs.copy();
			NMatrix[] outputs = new NMatrix[this.weights.length-1];
			
			for(int i = 0; i < this.weights.length; i++) {
				NMatrix hiddenLayer = NMatrix.multiply(this.weights[i], current);
				current = NMatrix.map(hiddenLayer, this, this.activation);
				outputs[i] = current.copy();
			}
			
			// Calculate error
			// This is target - output
			NMatrix outputErrors = NMatrix.subtract(targets, outputs);
			
			// Commence back propagation
			NMatrix error = new NMatrix(3);
			for(int i = weights.length-1; i >= 0; i--) {
				NMatrix hidden_T = this.weights[i].transpose();
				errors = NMatrix.multiply(hidden_T, outputErrors);
				NMatrix grad = NMatrix.map(outputs[i], this, this.derivative);
			}
			
			// Commence back propagation (Output <-> Hidden)
			// Transpose hidden to output layer
			NMatrix wHO_T = this.wHO.transpose();
			
			// Multiply output error by weights (wHO) for hidden errors
			NMatrix hiddenErrors = NMatrix.multiply(wHO_T, outputErrors);
			
			// Calculate the gradient of high dimensional surface...
			NMatrix grad_out = NMatrix.map(outputs, this, this.derivative);
			
			// Weigh by error and learning rate
			grad_out = NMatrix.dotMultiply(grad_out, outputErrors);
			grad_out = grad_out.scale(this.learningRate);
			
			// Continue back propagating (Hidden <-> Input)
			NMatrix grad_hidden = NMatrix.map(hiddenOutputs, this, this.derivative);
			grad_hidden = NMatrix.dotMultiply(grad_hidden, hiddenErrors);
			grad_hidden = grad_hidden.scale(this.learningRate);
			
			// Change weights
			// Calculate delta (Input -> Hidden)
			NMatrix inputs_T = inputs.transpose();
			NMatrix delta_hidden = NMatrix.multiply(grad_hidden, inputs_T);
			this.wIH = NMatrix.add(this.wIH, delta_hidden); // Update 
			
			// Calculate delta (Hidden -> Output)
			NMatrix hidden_outputs_T = hiddenOutputs.transpose();
			NMatrix delta_output = NMatrix.multiply(grad_out, hidden_outputs_T);
			this.wHO = NMatrix.add(this.wHO, delta_output);
		} catch (Exception e) {
			System.out.println("Fatal Error occoured: contact developer!");
			e.printStackTrace();
		}
	}
}